{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/silviaruiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/silviaruiz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from datetime import datetime, timedelta\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Importing libraries you need to install\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "import bs4 as bs\n",
    "from lxml import html\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "\n",
    "#import yfinance and pandas_datareader\n",
    "from pandas_datareader import data as pdr \n",
    "import yfinance as yf \n",
    "\n",
    "#override function to store data we get\n",
    "yf.pdr_override()\n",
    "\n",
    "#import nltk for first step extracting words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#set up stop_words from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "\"\"\"\n",
    "this is where different from version 1\n",
    "\"\"\"\n",
    "#import libraries for n-gram counting\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we have all the txts stored in the file:\n",
    "'./Scrape_data/Scrape_data/10_k/[cik]/grabbed_text/[cik]_[date]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can make a dictionary to store all the data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agilent Technologies Inc.</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Group Inc.</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advance Auto Parts Inc.</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AbbVie Inc.</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Xylem Inc.</td>\n",
       "      <td>XYL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Yum! Brands Inc.</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Zimmer Biomet Holdings Inc.</td>\n",
       "      <td>ZBH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Zions Bancorporation N.A.</td>\n",
       "      <td>ZION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Zoetis Inc. Class A</td>\n",
       "      <td>ZTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             name ticker\n",
       "0       Agilent Technologies Inc.      A\n",
       "1    American Airlines Group Inc.    AAL\n",
       "2         Advance Auto Parts Inc.    AAP\n",
       "3                      Apple Inc.   AAPL\n",
       "4                     AbbVie Inc.   ABBV\n",
       "..                            ...    ...\n",
       "500                    Xylem Inc.    XYL\n",
       "501              Yum! Brands Inc.    YUM\n",
       "502   Zimmer Biomet Holdings Inc.    ZBH\n",
       "503     Zions Bancorporation N.A.   ZION\n",
       "504           Zoetis Inc. Class A    ZTS\n",
       "\n",
       "[505 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#read the ticker library of all the tikers into ticker_library\n",
    "ticker_library = pd.read_csv('tickers.csv')\n",
    "\n",
    "#read the sp500 components into ticker_selected, 'name' is the company name and ticker is company's ticker\n",
    "ticker_selected = pd.read_csv('SP500_component_stocks.csv',header = None)\n",
    "ticker_selected.columns = ['name','ticker']\n",
    "\n",
    "#display a sample of ticker_selected\n",
    "display(ticker_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001090872</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006201</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001158449</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000320193</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001551152</td>\n",
       "      <td>ABBV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik ticker\n",
       "0  0001090872      A\n",
       "1  0000006201    AAL\n",
       "2  0001158449    AAP\n",
       "3  0000320193   AAPL\n",
       "4  0001551152   ABBV"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a ticker_cik_df dataframe to store ticker and its cik number\n",
    "ticker_cik_df = pd.DataFrame()\n",
    "\n",
    "#store all the tickers in a ticker_list\n",
    "ticker_list = ticker_selected.ticker.unique()\n",
    "\n",
    "#build a list cik_list for cik\n",
    "cik_list = []\n",
    "\n",
    "for ticker in ticker_list:    \n",
    "    try:\n",
    "        #for a given ticker, find its cik number through th ticker library\n",
    "        cik_list.append(list(ticker_library[ticker_library.ticker == ticker].secfilings)[0][-10:])\n",
    "        \n",
    "    except:\n",
    "        #if could not find cik, give it a empty cik\n",
    "        cik_list.append('')\n",
    "\n",
    "#write cik_list and ticker_list to the dataframe ticker_cik_df\n",
    "ticker_cik_df['cik'] = cik_list\n",
    "ticker_cik_df['ticker'] = ticker_list\n",
    "\n",
    "#delete the tickers with empty cik number\n",
    "ticker_cik_df = ticker_cik_df[ticker_cik_df['cik'] != '']\n",
    "\n",
    "#display a sample of ticker_cik_df\n",
    "ticker_cik_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a sample to analysis the overall program\n",
    "#use a sample to analysis the overall program\n",
    "# sample1 = ticker_cik_df.iloc[0:50]\n",
    "# sample2 = ticker_cik_df.iloc[50:100]\n",
    "# sample3 = ticker_cik_df.iloc[100:150]\n",
    "# sample4 = ticker_cik_df.iloc[150:200]\n",
    "# sample5 = ticker_cik_df.iloc[200:250]\n",
    "# sample6 = ticker_cik_df.iloc[250:300]\n",
    "# sample7 = ticker_cik_df.iloc[300:350]\n",
    "# sample8 = ticker_cik_df.iloc[350:400]\n",
    "# sample9 = ticker_cik_df.iloc[400:450]\n",
    "# sample10 = ticker_cik_df.iloc[450:500]\n",
    "ticker_cik_sample = ['0000049826','0000816284']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data using dictionary\n",
    "all_data = {}\n",
    "\n",
    "#set the key of dictionary as ticker\n",
    "for cik, ticker in zip(cik_list, ticker_list):\n",
    "    \n",
    "    #set the value of tikcer as a dict\n",
    "    all_data[ticker] = {}\n",
    "\n",
    "    #set the dict data[ticker] \n",
    "    all_data[ticker]['cik'] = cik\n",
    "    all_data[ticker]['10ks'] = {}\n",
    "    all_data[ticker]['10qs'] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['ITW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname_10k = './Scrape_data/10_k/'\n",
    "pathname_10q = './Scrape_data/10_q/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where different from version 1, we use the different length of days. 3 Month is an indication of long term impact.\n",
    "\"\"\"\n",
    "\n",
    "# Define a function to get stock prices from Yahoo API, and calculate the return the return of stocks\n",
    "def Get_Ex_Ret(ticker,fillingdate):\n",
    "    \n",
    "    #give a start date\n",
    "    start = datetime.strptime(fillingdate, '%Y-%m-%d')\n",
    "    \n",
    "    \"\"\"\n",
    "    change from 10 to 120 to ensure engough days\n",
    "    \"\"\"\n",
    "    #end date is 120 days after to ensure we have enough days\n",
    "    end = start + timedelta(120)\n",
    "    \n",
    "    #get 120 days' data\n",
    "    price_120_days = pdr.get_data_yahoo(ticker, start, end)\n",
    "    \n",
    "    \"\"\"\n",
    "    change from 5 days to 63 days---which is the length of three month\n",
    "    \"\"\"\n",
    "    #culculate excess returns of 63 days\n",
    "    ret_4d = price_120_days.iloc[62]['Adj Close'] / price_120_days.iloc[0]['Adj Close'] - 1\n",
    "    return 100 * ret_4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to remove punctuations if a given word ended with a punctuation\n",
    "def remove_punct(string):\n",
    "    return re.sub(r\"[{}]+\".format(punctuation), \"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function for filtering words\n",
    "def filter_words(string):\n",
    "    return bool(re.match(r'^[a-z\\']+$', string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where different from version 1, adding a beta term to calculate excess return.\n",
    "\"\"\"\n",
    "\n",
    "#preprocess the data and store them\n",
    "for ticker in all_data.keys():\n",
    "    \n",
    "    #give the cik for this ticker\n",
    "    cik = all_data[ticker]['cik']\n",
    "    \n",
    "    #goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        files_10k = os.listdir(pathname_10k + cik + '/grabbed_text/')\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    #iterate through the 10k path and get the information and txt\n",
    "    for file_10k in files_10k:\n",
    "        \n",
    "        #get the release time\n",
    "        release_10k = file_10k[-14:-4]\n",
    "        \n",
    "        \"\"\"\n",
    "        adding beta = 1, denoting the market return value at that time to calculate excess return.\n",
    "        \"\"\"\n",
    "        \n",
    "        #get the 5 day excess return for the given release time\n",
    "        try:\n",
    "            excess_return = Get_Ex_Ret(ticker, release_10k)\n",
    "            \"\"\"\n",
    "            adding a market return term and subtracting it(assuming beta = 1)\n",
    "            \"\"\"\n",
    "            market_return = Get_Ex_Ret('SPY', release_10k)\n",
    "            excess_return = excess_return - market_return\n",
    "            \n",
    "        #exception may happen if we don't have the ticker name in yahoo finance, so we simply delete it    \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        #preprocess the txt and store it in a list of words\n",
    "        #open the text file and read file as lower string\n",
    "        with open(pathname_10k + cik + '/grabbed_text/' + file_10k, encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "        \n",
    "        #rule out all the stop words and store them in a list. Also rule out all the puctuations\n",
    "        filtered_words = [remove_punct(word) for word in string_temp.split() if word not in stop_words]\n",
    "        \n",
    "        #filter word for all words\n",
    "        filtered_words = list(filter(filter_words, filtered_words))\n",
    "        \n",
    "        #reduce word to its root form\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'v') for word in filtered_words]\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'n') for word in filtered_words]\n",
    "        \n",
    "        #remove all empty values\n",
    "        #while '' in filtered_words:\n",
    "            #filtered_words.remove('')\n",
    "        \n",
    "        #store all the information needed in the all_data dict\n",
    "        if filtered_words != []:\n",
    "            all_data[ticker]['10ks'][release_10k] = {}\n",
    "            all_data[ticker]['10ks'][release_10k]['ex_return'] = excess_return\n",
    "            all_data[ticker]['10ks'][release_10k]['words'] = ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where different from version 1, adding a beta term to calculate excess return.\n",
    "\"\"\"\n",
    "\n",
    "for ticker in all_data.keys():\n",
    "    \n",
    "    #give the cik for this ticker\n",
    "    cik = all_data[ticker]['cik']\n",
    "    \n",
    "    #goes into the directory to find the path for txtfiles\n",
    "    try:\n",
    "        files_10q = os.listdir(pathname_10q + cik + '/grabbed_text/')\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "    #iterate through the 10q path and get the information and txt\n",
    "    for file_10q in files_10q:\n",
    "        \n",
    "        #get the release time\n",
    "        release_10q = file_10q[-14:-4]       \n",
    "        \n",
    "        \"\"\"\n",
    "        adding beta = 1, denoting the market return value at that time to calculate excess return.\n",
    "        \"\"\"\n",
    "        \n",
    "        #get the 5 day excess return for the given release time\n",
    "        try:\n",
    "            excess_return = Get_Ex_Ret(ticker, release_10q)\n",
    "            \"\"\"\n",
    "            adding a market return term and subtracting it(assuming beta = 1)\n",
    "            \"\"\"\n",
    "            market_return = Get_Ex_Ret('SPY', release_10k)\n",
    "            excess_return = excess_return - market_return\n",
    "        \n",
    "        #exception may happen if we don't have the ticker name in yahoo finance, so we simply delete it    \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "        #preprocess the txt and store it in a list of words\n",
    "        #open the text file and read file as lower string\n",
    "        with open(pathname_10q + cik + '/grabbed_text/' + file_10q, encoding = \"utf8\") as f:\n",
    "            string_temp = f.read().lower()\n",
    "        \n",
    "        #rule out all the stop words and store them in a list. Also rule out all the puctuations\n",
    "        filtered_words = [remove_punct(word) for word in string_temp.split() if word not in stop_words]\n",
    "        \n",
    "        #filter word for all words\n",
    "        filtered_words = list(filter(filter_words, filtered_words))\n",
    "        \n",
    "        #reduce word to its root form\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'v') for word in filtered_words]\n",
    "        filtered_words = [lemmatizer.lemmatize(word, pos = 'n') for word in filtered_words]\n",
    "        \n",
    "        #remove all empty values\n",
    "        #while '' in filtered_words:\n",
    "            #filtered_words.remove('')\n",
    "        \n",
    "        #store all the information needed in the all_data dict\n",
    "        if filtered_words != []:\n",
    "            all_data[ticker]['10qs'][release_10q] = {}\n",
    "            all_data[ticker]['10qs'][release_10q]['ex_return'] = excess_return\n",
    "            all_data[ticker]['10qs'][release_10q]['words'] = ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look at the data stucture now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data['NKE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze 10K word with tfidf and bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10ks and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10K documents we have\n",
    "document_num_10k = 0\n",
    "\n",
    "#word list for 10k \n",
    "word_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10k\n",
    "        document_num_10k += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10ks'][date]['tf'] = Counter(all_data[ticker]['10ks'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10k document of a given date\n",
    "        for word in all_data[ticker]['10ks'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10k[word] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf value for the word in 10ks\n",
    "idf_10k = {}\n",
    "\n",
    "#iterate through all the words in word_list_10k\n",
    "for word in word_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k[word] = np.log(document_num_10k / (1 + word_list_10k[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word pair list for 10k\n",
    "pair_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "#         #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "#         print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #print(all_data[ticker]['10ks'][date]['tf_pair'])\n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10k document of a given date\n",
    "        for pair in all_data[ticker]['10ks'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10k[pair] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_list_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10ks\n",
    "idf_10k_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10k\n",
    "for pair in pair_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_pair[pair] = np.log(document_num_10k / (1 + pair_list_10k[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10k files\n",
    "# document_num_10k = document_num_10k\n",
    "\n",
    "#word triple list for 10k\n",
    "triple_list_10k = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        #there is no need for counting document_num_10k\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10ks'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10ks'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10k document of a given date\n",
    "        for triple in all_data[ticker]['10ks'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10k[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10ks\n",
    "idf_10k_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10k\n",
    "for triple in triple_list_10k:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10k_triple[triple] = np.log(document_num_10k / (1 + triple_list_10k[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doing the same to 10qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also do the same to 10q files\n",
    "\n",
    "\"\"\"\n",
    "This part is to compute the tf values for the words in 10qs and collect the overall word list for computing idf in the next step\n",
    "\"\"\"\n",
    "#count the number of 10Q documents we have\n",
    "document_num_10q = 0\n",
    "\n",
    "#word list for 10q \n",
    "word_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #we have document for a given date, so add 1 for document_num_10q\n",
    "        document_num_10q += 1\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        all_data[ticker]['10qs'][date]['tf'] = Counter(all_data[ticker]['10qs'][date]['words'].split())\n",
    "        \n",
    "        #iterate through the words in tf, which is the words of a given 10q document of a given date\n",
    "        for word in all_data[ticker]['10qs'][date]['tf']:\n",
    "            \n",
    "            #add one if it already contains the word, or add the this word to the dict if not\n",
    "            word_list_10q[word] += 1\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute idf value for the word in 10qs\n",
    "idf_10q = {}\n",
    "\n",
    "#iterate through all the words in word_list_10q\n",
    "for word in word_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q[word] = np.log(document_num_10q / (1 + word_list_10q[word]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word pair list for 10q\n",
    "pair_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (2, 2))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_pair'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the pairs in tf_pair, which is the words of a given 10q document of a given date\n",
    "        for pair in all_data[ticker]['10qs'][date]['tf_pair']:\n",
    "            \n",
    "            #add one if it already contains the pair, or add the this pair to the dict if not have\n",
    "            pair_list_10q[pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the pair in 10qs\n",
    "idf_10q_pair = {}\n",
    "\n",
    "#iterate through all the pairs in pair_list_10q\n",
    "for pair in pair_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_pair[pair] = np.log(document_num_10q / (1 + pair_list_10q[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#we already have the number of all 10q files\n",
    "# document_num_10q = document_num_10q\n",
    "\n",
    "#word triple list for 10q\n",
    "triple_list_10q = defaultdict(int)\n",
    "\n",
    "\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        #there is no need for counting document_num_10q\n",
    "        \n",
    "        #compute the tfs for the txt file\n",
    "        \"\"\"\n",
    "        The only adaptation from pair to triple is changing ngram_range, needing futher simplification of code\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(analyzer = 'word', ngram_range = (3, 3))\n",
    "        \n",
    "        #fit it through vectorizer\n",
    "        fitted = vectorizer.fit_transform([all_data[ticker]['10qs'][date]['words']])\n",
    "        \n",
    "        #after vectorizer, we have feature name and feature count, feed them to a dataframe\n",
    "        df_temp = pd.DataFrame(index = vectorizer.get_feature_names(), data = np.squeeze(fitted.toarray()))\n",
    "        \n",
    "        #then we can add the data into the main data set all_data\n",
    "        all_data[ticker]['10qs'][date]['tf_triple'] = df_temp.to_dict()[0]\n",
    "        \n",
    "        #iterate through the triples in tf_triple, which is the words of a given 10q document of a given date\n",
    "        for triple in all_data[ticker]['10qs'][date]['tf_triple']:\n",
    "            \n",
    "            #add one if it already contains the triple, or add the this triple to the dict if not have\n",
    "            triple_list_10q[triple] += 1\n",
    "            \n",
    "\"\"\"\n",
    "This part is new in the version 2. It's using for n-gram preparation.\n",
    "\"\"\"\n",
    "\n",
    "#compute idf value for the triple in 10qs\n",
    "idf_10q_triple = {}\n",
    "\n",
    "#iterate through all the triples in triple_list_10q\n",
    "for triple in triple_list_10q:\n",
    "    \n",
    "    #compute idf value\n",
    "    idf_10q_triple[triple] = np.log(document_num_10q / (1 + triple_list_10q[triple]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### have a look at the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, containing idfs for word pairs\n",
    "\"\"\"\n",
    "\n",
    "idf_10q_triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the data for future use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this part is for storing the data for future use\n",
    "\"\"\"\n",
    "#delete word in all_data for storage\n",
    "#iterate through the tickers\n",
    "for ticker in all_data:\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10qs']:\n",
    "        \n",
    "        del all_data[ticker]['10qs'][date]['words']\n",
    "    \n",
    "    #for a given ticker, iterate through date\n",
    "    for date in all_data[ticker]['10ks']:\n",
    "        \n",
    "        del all_data[ticker]['10ks'][date]['words']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write all_data to a json file\n",
    "with open('all_data.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(all_data))\n",
    "\n",
    "#write idf_10k to a json file\n",
    "with open('idf_10k.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k))\n",
    "\n",
    "#write idf_10q to a json file    \n",
    "with open('idf_10q.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is new in version 2, storing files for word pairs and word triples\n",
    "\"\"\"\n",
    "#write idf_10k_pair to a json file  \n",
    "with open('idf_10k_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_pair))\n",
    "\n",
    "#write idf_10q_pair to a json file    \n",
    "with open('idf_10q_pair.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_pair))\n",
    "    \n",
    "#write idf_10k_triple to a json file  \n",
    "with open('idf_10k_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10k_triple))\n",
    "\n",
    "#write idf_10q_triple to a json file    \n",
    "with open('idf_10q_triple.json', 'w') as json_file:\n",
    "    json_file.write(json.dumps(idf_10q_triple))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
